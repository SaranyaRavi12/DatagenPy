{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "README\n",
    ">>>>>Generate Mock Data Files based on schema\n",
    ">>>>>Generate Master Files Party, Account with record count greater than the mock data files\n",
    ">>>>>Replace Party , Account Number Details with Master Files to enforce RI if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REPLACE PARTY,ACCOUNT WITH MASTER FOR JOINING#\n",
    "\n",
    "import pandas as pd\n",
    "from tkinter import Tk\n",
    "from tkinter.filedialog import askopenfilename\n",
    "\n",
    "# Function to browse and select a file\n",
    "def browse_file(prompt):\n",
    "    Tk().withdraw()  # We don't want a full GUI, so keep the root window from appearing\n",
    "    filename = askopenfilename(title=prompt)\n",
    "    return filename\n",
    "\n",
    "# Get user inputs for source file, target file, and column name\n",
    "source = browse_file(\"Select the source CSV file\")\n",
    "target = browse_file(\"Select the target CSV file\")\n",
    "\n",
    "key_column = input(\"Enter the column name to check for duplicates (e.g., 'ACCT_NUM'): \").lower()\n",
    "\n",
    "# Load the two CSV files with the specified column as a string\n",
    "df_source = pd.read_csv(source, dtype={key_column: str})\n",
    "df_target = pd.read_csv(target, dtype={key_column: str})\n",
    "\n",
    "# Convert column names to lowercase\n",
    "df_source.columns = df_source.columns.str.lower()\n",
    "df_target.columns = df_target.columns.str.lower()\n",
    "\n",
    "# Remove duplicates based on the key column\n",
    "df_unique_source = df_source.drop_duplicates(subset=[key_column])\n",
    "df_unique_target = df_target.drop_duplicates(subset=[key_column])\n",
    "\n",
    "# Replace the column values only if target count is less than source count\n",
    "if len(df_unique_target) <= len(df_unique_source):\n",
    "    df_unique_target[key_column] = df_unique_source[key_column].values[:len(df_unique_target)]\n",
    "else:\n",
    "    raise ValueError(\"Target count is greater than source count. Replacement not possible.\")\n",
    "\n",
    "\n",
    "# Save the updated DataFrame back to a CSV file\n",
    "output_file = 'updated_' + target.split('/')[-1]\n",
    "df_unique_target.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Column values replaced successfully! The updated file is saved as '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged DataFrame:\n",
      "             party_num          party_name address_line    acct_num  \\\n",
      "0   459687844127169650          James Cole      unknown  6276363016   \n",
      "1   242366242910176282         Lee Collins      unknown  3386081460   \n",
      "2   355033838990046324        Randy Garcia      unknown  8843539658   \n",
      "3   753504545835546895      Matthew Norris      unknown  8918063362   \n",
      "4   205372666969557920      Kelsey Roberts      unknown  4566468088   \n",
      "5   845669279428142160        Joseph Davis      unknown  8227832189   \n",
      "6   345050757437515908      Bonnie Johnson      unknown  5684371681   \n",
      "7   205036380121880285      Jennifer Moody      unknown  1780784924   \n",
      "8   619806098608840618        Patrick Howe      unknown  3178535577   \n",
      "9   354506827549611183    Kimberly Osborne      unknown  4488710451   \n",
      "10  363881710539249517  Katherine Martinez      unknown  8465899971   \n",
      "11  794815621605814551    Kristopher Ayala      unknown  6148855403   \n",
      "12  576739727161994012   Kimberly Williams      unknown  6755980214   \n",
      "13  813063884653988968         Gary Austin      unknown  7914308141   \n",
      "14  327058730761044771        Anthony Hall      unknown  9517493123   \n",
      "15  547650589920628647       Fernando Kemp      unknown  6386622213   \n",
      "16  293909676188691339        Morgan Avery      unknown  5446384640   \n",
      "17  208581866693412730   Christine Bradley      unknown  6261669799   \n",
      "18  986509080498541299  Theresa Washington      unknown  8323197576   \n",
      "19  802656878380069294        Sergio Miles      unknown  9400134284   \n",
      "20  802438639717095074        Kelly Wilson      unknown  9795476892   \n",
      "21  311503957470946806     Regina Peterson      unknown  7236142874   \n",
      "22  788102399382991247          Maria Ward      unknown  5325478224   \n",
      "23  855047980940922823       Dennis Pineda      unknown  2829137059   \n",
      "24  125458483870229553     Cheryl Williams      unknown  3564102006   \n",
      "25  886395020113928908      Jeffrey Fowler      unknown  8022722452   \n",
      "26  180390972164373457     Mr. Jason Boone      unknown  4568238594   \n",
      "27  398376185796197912        Randy Garcia      unknown  6097320414   \n",
      "28  975147151311963496      Nicole Jackson      unknown  8827834553   \n",
      "29  432168376803322607       Julie Watkins      unknown  3489427531   \n",
      "30  654184259847836757    Patrick Johnston      unknown  3064180528   \n",
      "31  119263836943546658       Andrea Fuller      unknown  3213921813   \n",
      "32  109161825649036370          Ryan Clark      unknown  7753690532   \n",
      "33  472394856647244042        Joseph Grant      unknown  5693876480   \n",
      "34  453152363787563446       Laura Johnson      unknown  9519259792   \n",
      "35  922301479688992960      Meghan Johnson      unknown  7060036977   \n",
      "36  915156763101054755         Tammy Davis      unknown  7326120346   \n",
      "37  885384515597064494   Katrina Alexander      unknown  9795832982   \n",
      "38  658585352976016388       Barbara Davis      unknown  2385188224   \n",
      "39  530903372119059660     Barbara Jackson      unknown  2970464409   \n",
      "40  629103986349133739        Rachel Moore      unknown  1107580761   \n",
      "41  337758662642988044          Laura Rios      unknown  4224731234   \n",
      "42  638952325884287160     Michelle Dalton      unknown  2748076119   \n",
      "43  119963418845504746           Dawn Tate      unknown  6781182461   \n",
      "44  170429221226266848   Ms. Emily Preston      unknown  5108121808   \n",
      "45  784546233280128483     Justin Mitchell      unknown  3495542762   \n",
      "46  882742557239800216      Robert Jenkins      unknown  7330527120   \n",
      "47  724629355088131643     Kevin Henderson      unknown  5710223182   \n",
      "48  624912569127155456    Katherine Holden      unknown  5870384268   \n",
      "49  544370561772122464        Andrew Perry      unknown  9163413660   \n",
      "\n",
      "         role               acct_name      sys_dt                 time  ind  \\\n",
      "0   Secondary          Benjamin Mckay  2005-12-07  1988-01-03T12:01:09    0   \n",
      "1   Secondary            Kathryn Reed  2011-12-22  2023-07-04T10:42:14    1   \n",
      "2   Secondary           Melinda Beard  1990-04-06  2006-09-14T10:07:39    0   \n",
      "3     Primary        Heather Anderson  1997-01-19  2020-06-27T15:27:45    1   \n",
      "4     Primary             Kevin Bruce  2015-10-02  1999-11-27T23:57:00    0   \n",
      "5   Secondary              Kayla Rose  1975-11-22  1973-12-24T23:40:08    0   \n",
      "6   Secondary           Lisa Anderson  1983-04-13  1997-10-23T20:03:55    1   \n",
      "7   Secondary              Carol Koch  2023-08-08  1992-11-13T14:56:20    1   \n",
      "8   Secondary              Jose Lewis  2009-07-27  1981-10-25T20:08:02    1   \n",
      "9   Secondary            Daniel Gomez  1998-04-29  1977-09-20T06:06:23    1   \n",
      "10    Primary          Jeffery Morrow  1991-10-21  2017-12-22T07:13:44    1   \n",
      "11    Primary            Angela Baker  2004-10-29  2024-02-05T15:13:44    1   \n",
      "12    Primary            Kathy Miller  2010-09-21  2001-07-31T05:10:49    0   \n",
      "13  Secondary             Joel Harris  1972-09-20  2020-06-02T04:46:53    1   \n",
      "14  Secondary  Mr. Charles Lowery DDS  1977-06-15  2018-03-01T05:57:14    1   \n",
      "15  Secondary               Laura Lee  1981-04-17  1970-11-17T00:06:15    0   \n",
      "16    Primary           Sheila Barton  1996-04-03  2007-01-08T10:16:09    0   \n",
      "17  Secondary              Kyle Potts  1976-04-25  1970-12-02T09:54:04    0   \n",
      "18  Secondary       Jennifer Thompson  1999-05-10  2019-06-03T14:30:55    1   \n",
      "19    Primary         Danielle Vaughn  1978-03-06  1993-02-21T06:43:08    1   \n",
      "20    Primary           Wendy Baldwin  2001-11-04  1990-07-30T08:36:23    1   \n",
      "21    Primary           Caitlin Lewis  1985-05-01  2022-08-29T17:05:22    0   \n",
      "22    Primary            Brittney Cox  2024-05-11  2004-11-11T01:45:30    0   \n",
      "23  Secondary            Joshua Brown  2010-05-15  2011-08-18T13:21:18    0   \n",
      "24    Primary      Rebekah Fitzgerald  2000-02-17  1997-10-16T23:40:21    0   \n",
      "25  Secondary          Robert Hampton  1989-10-22  2003-11-21T18:33:46    1   \n",
      "26    Primary           Jeanne Carson  2023-07-07  1992-05-24T07:51:06    1   \n",
      "27  Secondary          Robert Rosario  2018-09-13  2022-02-21T03:16:42    1   \n",
      "28    Primary          Richard Harris  1974-09-30  2017-03-17T01:20:18    0   \n",
      "29  Secondary           Jake Martinez  1995-06-01  1998-12-01T13:26:05    1   \n",
      "30    Primary         Jonathan Galvan  2016-12-21  1981-07-06T18:40:13    0   \n",
      "31  Secondary            Carlos Smith  1990-05-03  1982-02-13T07:21:13    0   \n",
      "32    Primary          Lauren Vincent  1971-02-18  1985-12-01T16:00:27    0   \n",
      "33    Primary               Lee Lopez  2006-11-27  1974-09-04T16:37:57    0   \n",
      "34  Secondary            Jerome Perez  2022-03-06  2014-06-01T12:38:58    0   \n",
      "35  Secondary              Tyler Ball  1996-03-08  2006-11-11T06:11:25    0   \n",
      "36  Secondary        Chelsea Gonzales  2007-10-30  1990-07-22T14:50:52    0   \n",
      "37  Secondary          Anthony Meyers  1990-05-03  2024-11-25T01:20:03    1   \n",
      "38  Secondary             Paul Jacobs  1994-04-13  2000-02-22T11:45:24    0   \n",
      "39  Secondary             Amber House  1977-08-24  1982-03-07T08:21:19    1   \n",
      "40    Primary           Judith Taylor  2000-07-20  1970-05-22T23:46:46    0   \n",
      "41    Primary           Ashley Nelson  2018-05-03  2020-02-16T00:40:19    0   \n",
      "42  Secondary        Michael Johnston  1999-11-25  1971-03-12T06:36:24    0   \n",
      "43  Secondary         Jennifer Curtis  2013-02-24  1975-01-07T12:23:56    1   \n",
      "44    Primary          Pedro Villegas  2019-08-22  1994-05-08T16:37:41    0   \n",
      "45  Secondary                Sean Fox  2007-10-01  2005-03-10T02:55:31    0   \n",
      "46  Secondary           Michael Lopez  1996-01-26  2018-10-02T20:26:55    0   \n",
      "47    Primary             Sara Holmes  2023-11-01  1984-07-05T22:23:51    1   \n",
      "48    Primary       Patrick Wilkerson  1999-10-20  1985-01-07T08:21:53    1   \n",
      "49  Secondary         Thomas Mccarthy  2019-09-01  2016-04-11T16:32:58    0   \n",
      "\n",
      "   acct_ind  \n",
      "0    CLOSED  \n",
      "1    CLOSED  \n",
      "2      OPEN  \n",
      "3    CLOSED  \n",
      "4      OPEN  \n",
      "5      OPEN  \n",
      "6    CLOSED  \n",
      "7    CLOSED  \n",
      "8    CLOSED  \n",
      "9      OPEN  \n",
      "10     OPEN  \n",
      "11   CLOSED  \n",
      "12     OPEN  \n",
      "13   CLOSED  \n",
      "14   CLOSED  \n",
      "15   CLOSED  \n",
      "16     OPEN  \n",
      "17   CLOSED  \n",
      "18     OPEN  \n",
      "19     OPEN  \n",
      "20   CLOSED  \n",
      "21   CLOSED  \n",
      "22   CLOSED  \n",
      "23     OPEN  \n",
      "24     OPEN  \n",
      "25     OPEN  \n",
      "26   CLOSED  \n",
      "27     OPEN  \n",
      "28   CLOSED  \n",
      "29   CLOSED  \n",
      "30   CLOSED  \n",
      "31   CLOSED  \n",
      "32     OPEN  \n",
      "33   CLOSED  \n",
      "34   CLOSED  \n",
      "35   CLOSED  \n",
      "36     OPEN  \n",
      "37   CLOSED  \n",
      "38     OPEN  \n",
      "39   CLOSED  \n",
      "40     OPEN  \n",
      "41   CLOSED  \n",
      "42     OPEN  \n",
      "43     OPEN  \n",
      "44     OPEN  \n",
      "45   CLOSED  \n",
      "46     OPEN  \n",
      "47     OPEN  \n",
      "48   CLOSED  \n",
      "49   CLOSED  \n"
     ]
    }
   ],
   "source": [
    "##VALIDATOR##\n",
    "import pandas as pd\n",
    "from tkinter import Tk\n",
    "from tkinter.filedialog import askopenfilename\n",
    "\n",
    "# Function to browse and select a file\n",
    "def browse_file(prompt):\n",
    "    Tk().withdraw()  # We don't want a full GUI, so keep the root window from appearing\n",
    "    filename = askopenfilename(title=prompt)\n",
    "    return filename\n",
    "\n",
    "# Get user inputs for the three files and column names\n",
    "party_file = browse_file(\"Select the party CSV file\")\n",
    "party_account_file = browse_file(\"Select the party account CSV file\")\n",
    "#account_file = browse_file(\"Select the account CSV file\")\n",
    "\n",
    "party_key_column = input(\"Enter the column name to join party and party account files on (e.g., 'party_num'): \").lower()\n",
    "#account_key_column = input(\"Enter the column name to join party account and account files on (e.g., 'acct_num'): \").lower()\n",
    "\n",
    "# Load the three CSV files with the specified columns as strings\n",
    "df_party = pd.read_csv(party_file, dtype={party_key_column: str})\n",
    "df_party_account = pd.read_csv(party_account_file, dtype={party_key_column: str, account_key_column: str})\n",
    "#df_account = pd.read_csv(account_file, dtype={account_key_column: str})\n",
    "\n",
    "# Convert column names to lowercase\n",
    "df_party.columns = df_party.columns.str.lower()\n",
    "df_party_account.columns = df_party_account.columns.str.lower()\n",
    "df_account.columns = df_account.columns.str.lower()\n",
    "\n",
    "# Perform inner join on the key columns\n",
    "df_merged1 = df_party.merge(df_party_account, on=party_key_column)\n",
    "df_merged2 = df_merged1.merge(df_account, on=account_key_column)\n",
    "\n",
    "# Show the output\n",
    "print(\"Merged DataFrame:\")\n",
    "print(df_merged2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##GENERATE TEST DATA###\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import random\n",
    "import string\n",
    "from faker import Faker\n",
    "\n",
    "faker = Faker()\n",
    "\n",
    "# Function to generate random string of a given length\n",
    "def random_string(length):\n",
    "    return ''.join(random.choices(string.ascii_lowercase, k=length))\n",
    "\n",
    "# Function to generate random integer with a specific number of digits\n",
    "def random_integer(min_digits, max_digits):\n",
    "    min_value = 10**(min_digits - 1)\n",
    "    max_value = 10**max_digits - 1\n",
    "    return np.random.randint(min_value, max_value + 1, dtype=np.int64)\n",
    "\n",
    "# Function to generate unique IDs\n",
    "def generate_unique_ids(num_rows):\n",
    "    return list(range(1, num_rows + 1))\n",
    "\n",
    "# Function to generate data based on schema\n",
    "def generate_data(schema, num_rows):\n",
    "    data = {}\n",
    "    for index, row in schema.iterrows():\n",
    "        col = row[0]\n",
    "        dtype_info = row[1]\n",
    "        is_primary_key = len(row) > 2 and row[2] == 'primary_key'\n",
    "        dtype_parts = dtype_info.split(':')\n",
    "        dtype = dtype_parts[0]\n",
    "        min_length = int(dtype_parts[1]) if len(dtype_parts) > 1 else None\n",
    "        max_length = int(dtype_parts[2]) if len(dtype_parts) > 2 else None\n",
    "        permissible_values = dtype_parts[3].split('|') if len(dtype_parts) > 3 else None\n",
    "        \n",
    "        if is_primary_key:\n",
    "            data[col] = generate_unique_ids(num_rows)\n",
    "        elif permissible_values:\n",
    "            data[col] = np.random.choice(permissible_values, num_rows)\n",
    "        elif dtype == 'int':\n",
    "            if min_length and max_length:\n",
    "                data[col] = [random_integer(min_length, max_length) for _ in range(num_rows)]\n",
    "            else:\n",
    "                data[col] = np.random.randint(1, 100, num_rows)\n",
    "        elif dtype == 'float':\n",
    "            data[col] = np.random.rand(num_rows) * 100\n",
    "        elif dtype == 'name':\n",
    "            data[col] = [faker.name() for _ in range(num_rows)]\n",
    "        elif dtype == 'date':\n",
    "            data[col] = [faker.date() for _ in range(num_rows)]\n",
    "        elif dtype == 'datetime':\n",
    "            data[col] = [faker.date_time().isoformat() for _ in range(num_rows)]\n",
    "        elif dtype == 'bool_1_0':\n",
    "            data[col] = np.random.choice([1, 0], num_rows)\n",
    "        elif dtype == 'str':\n",
    "            if min_length and max_length:\n",
    "                lengths = np.random.randint(min_length, max_length + 1, num_rows)\n",
    "                data[col] = [random_string(length) for length in lengths]\n",
    "            elif min_length:\n",
    "                data[col] = [random_string(min_length) for _ in range(num_rows)]\n",
    "            else:\n",
    "                data[col] = np.random.choice(['A', 'B', 'C'], num_rows)\n",
    "        else:\n",
    "            data[col] = np.random.choice(['unknown'], num_rows)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Load all transposed schema files\n",
    "schema_files = glob.glob('schema_*.csv')  # Adjust the pattern if needed\n",
    "# Generate and save test data for each schema file\n",
    "num_rows = 50\n",
    "for schema_file in schema_files:\n",
    "    schema = pd.read_csv(schema_file, header=None)\n",
    "    test_data = generate_data(schema, num_rows)\n",
    "    output_file = schema_file.replace('schema_', 'test_data_')  # Create corresponding output file name\n",
    "    test_data.to_csv(output_file, index=False)\n",
    "    print(f\"Test data generated and saved to '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##GENERATE MASTER DATA###\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Function to generate unique 18-digit party numbers\n",
    "def generate_unique_party_numbers(num_rows):\n",
    "    unique_numbers = set()\n",
    "    while len(unique_numbers) < num_rows:\n",
    "        unique_numbers.add(str(np.random.randint(10**17, 10**18, dtype=np.int64)))\n",
    "    return list(unique_numbers)\n",
    "\n",
    "# Function to generate unique 10-digit account numbers\n",
    "def generate_unique_account_numbers(num_rows):\n",
    "    unique_numbers = set()\n",
    "    while len(unique_numbers) < num_rows:\n",
    "        unique_numbers.add(str(np.random.randint(10**9, 10**10, dtype=np.int64)))\n",
    "    return list(unique_numbers)\n",
    "\n",
    "# Function to define roles\n",
    "def define_roles(num_rows):\n",
    "    roles = ['Primary', 'Secondary']\n",
    "    return np.random.choice(roles, num_rows)\n",
    "\n",
    "# Number of rows for each file\n",
    "num_rows_account = 50\n",
    "num_rows_party = num_rows_account * 2\n",
    "\n",
    "# Generate data\n",
    "party_numbers = generate_unique_party_numbers(num_rows_party)\n",
    "account_numbers = generate_unique_account_numbers(num_rows_account)\n",
    "roles = define_roles(num_rows_account)\n",
    "\n",
    "# Create DataFrames\n",
    "df_party = pd.DataFrame({'PARTY_NUM': party_numbers})\n",
    "df_account = pd.DataFrame({'ACCT_NUM': account_numbers})\n",
    "df_role = pd.DataFrame({'Role': roles})\n",
    "\n",
    "# Save DataFrames to CSV files\n",
    "df_party.to_csv('party_numbers.csv', index=False)\n",
    "df_account.to_csv('account_numbers.csv', index=False)\n",
    "df_role.to_csv('roles.csv', index=False)\n",
    "\n",
    "# Combine data into a single DataFrame ensuring no account, role combination have the same customer\n",
    "combined_data = []\n",
    "used_combinations = set()\n",
    "\n",
    "for i in range(num_rows_account):\n",
    "    account = account_numbers[i]\n",
    "    role = roles[i]\n",
    "    customer = party_numbers[i]\n",
    "    if (account, role) not in used_combinations:\n",
    "        combined_data.append({'Customer': customer, 'Account': account, 'Role': role})\n",
    "        used_combinations.add((account, role))\n",
    "\n",
    "# Add remaining party numbers to the combined data\n",
    "remaining_customers = set(party_numbers) - set([entry['Customer'] for entry in combined_data])\n",
    "for customer in remaining_customers:\n",
    "    account = np.random.choice(account_numbers)\n",
    "    role = np.random.choice(roles)\n",
    "    combined_data.append({'Customer': customer, 'Account': account, 'Role': role})\n",
    "\n",
    "# Ensure all account numbers are used at least once\n",
    "used_accounts = set([entry['Account'] for entry in combined_data])\n",
    "remaining_accounts = set(account_numbers) - used_accounts\n",
    "for account in remaining_accounts:\n",
    "    customer = np.random.choice(party_numbers)\n",
    "    role = np.random.choice(roles)\n",
    "    combined_data.append({'Customer': customer, 'Account': account, 'Role': role})\n",
    "\n",
    "# Ensure all party numbers are used at least once\n",
    "used_customers = set([entry['Customer'] for entry in combined_data])\n",
    "remaining_customers = set(party_numbers) - used_customers\n",
    "for customer in remaining_customers:\n",
    "    account = np.random.choice(account_numbers)\n",
    "    role = np.random.choice(roles)\n",
    "    combined_data.append({'Customer': customer, 'Account': account, 'Role': role})\n",
    "\n",
    "df_combined = pd.DataFrame(combined_data)\n",
    "\n",
    "# Save the combined DataFrame to a CSV file\n",
    "df_combined.to_csv('customer_account_role.csv', index=False)\n",
    "\n",
    "print(\"Files generated successfully!\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
