{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement faiss (from versions: none)\n",
      "ERROR: No matching distribution found for faiss\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##VERSION1.0\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from ollama import Client\n",
    "\n",
    "# Function to read web page content\n",
    "def read_web_page(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    # Remove unnecessary elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()\n",
    "    return soup.get_text()\n",
    "\n",
    "# Function to answer questions using ollama\n",
    "def answer_questions(content, question):\n",
    "    client = Client(host='http://localhost:11434')\n",
    "    chat_prompt = [{'role': 'user', 'content': f\"Here is some content: {content}\\n\\nBased on this content, please answer the following question: {question}\\n\\nAnswer:\"}]\n",
    "    response = client.chat(model='llama3.2', messages=chat_prompt)\n",
    "    return response['message']['content']\n",
    "\n",
    "def main():\n",
    "    urls = input(\"Enter URLs (comma separated): \").split(',')\n",
    "    question = input(\"Enter your question: \")\n",
    "    combined_content = ' '.join([read_web_page(url.strip()) for url in urls])\n",
    "    answer = answer_questions(combined_content, question)\n",
    "    print(f\"Answer: {answer}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##VERSION2.0\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from ollama import Client\n",
    "import os\n",
    "\n",
    "# Function to read web page content\n",
    "def read_web_page(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # Remove unnecessary elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "        return soup.get_text()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching the URL: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Function to answer questions using ollama\n",
    "def answer_questions(content, question):\n",
    "    client = Client(host='http://localhost:11434')\n",
    "    chat_prompt = [{'role': 'user', 'content': f\"Here is some content: {content}\\n\\nBased on this content, please answer the following question: {question}\\n\\nAnswer:\"}]\n",
    "    response = client.chat(model='llama3.2', messages=chat_prompt)\n",
    "    return response['message']['content']\n",
    "\n",
    "# Function to summarize content\n",
    "def summarize_content(content):\n",
    "    client = Client(host='http://localhost:11434')\n",
    "    chat_prompt = [{'role': 'user', 'content': f\"Summarize the following content: {content}\"}]\n",
    "    response = client.chat(model='llama3.2', messages=chat_prompt)\n",
    "    return response['message']['content']\n",
    "\n",
    "def main():\n",
    "    urls = input(\"Enter URLs (comma separated): \").split(',')\n",
    "    combined_content = ' '.join([read_web_page(url.strip()) for url in urls])\n",
    "    \n",
    "    if not combined_content.strip():\n",
    "        print(\"No valid content fetched from the provided URLs.\")\n",
    "        return\n",
    "    \n",
    "    summary = summarize_content(combined_content)\n",
    "    print(f\"Summary of the content: {summary}\")\n",
    "    \n",
    "    with open(\"questions_and_answers.txt\", \"a\") as file:\n",
    "        while True:\n",
    "            question = input(\"Enter your question: \")\n",
    "            answer = answer_questions(combined_content, question)\n",
    "            print(f\"Answer: {answer}\")\n",
    "            \n",
    "            file.write(f\"Question: {question}\\nAnswer: {answer}\\n\\n\")\n",
    "            \n",
    "            continue_prompt = input(\"Do you want to ask another question? (yes/no): \").strip().lower()\n",
    "            if continue_prompt != 'yes':\n",
    "                break\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: No, the pop-up blocker in Microsoft Edge cannot block malware. According to the text, \"If you still see pop-ups within a web page once this feature is turned on and you have tried the solutions listed above, they may be website advertisements created to look like pop-ups. Website advertisements cannot be blocked by the Edge pop-up blocker.\"\n",
      "Answer: The content does not explicitly state that pop-ups are \"good\" or \"bad\". It only discusses how to block pop-ups in Microsoft Edge and explains that some pop-ups can be helpful (e.g., displaying a bank statement), while others can be distracting or malicious. However, it also notes that website advertisements created to look like pop-ups cannot be blocked by the Edge pop-up blocker, suggesting that not all pop-ups are created equal.\n",
      "Answer: Yes, you can disable the pop-up blocker in Microsoft Edge. However, it's worth noting that disabling the pop-up blocker may expose your device to potential phishing scams or other malicious activities.\n",
      "\n",
      "To disable the pop-up blocker in Microsoft Edge:\n",
      "\n",
      "1. Go to Settings and more (three dots) at the top of your browser.\n",
      "2. Select \"Settings\" > Cookies and site permissions.\n",
      "3. Under \"All permissions,\" select \"Pop-ups and redirects.\"\n",
      "4. Turn off the toggle switch for \"Block (recommended)\".\n",
      "Answer: The CEO of Microsoft is not explicitly mentioned in the provided content. The content appears to be a support page for Microsoft Edge and does not include information about the current CEO of Microsoft.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from ollama import Client\n",
    "import os\n",
    "\n",
    "# Function to read web page content\n",
    "def read_web_page(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # Remove unnecessary elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "        return soup.get_text()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching the URL: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Function to answer questions using ollama\n",
    "def answer_questions(content, question):\n",
    "    client = Client(host='http://localhost:11434')\n",
    "    chat_prompt = [{'role': 'user', 'content': f\"Here is some content: {content}\\n\\nBased on this content, please answer the following question: {question}\\n\\nAnswer:\"}]\n",
    "    response = client.chat(model='llama3.2', messages=chat_prompt)\n",
    "    return response['message']['content']\n",
    "\n",
    "# Function to summarize content\n",
    "def summarize_content(content):\n",
    "    client = Client(host='http://localhost:11434')\n",
    "    chat_prompt = [{'role': 'user', 'content': f\"Summarize the following content: {content}\"}]\n",
    "    response = client.chat(model='llama3.2', messages=chat_prompt)\n",
    "    return response['message']['content']\n",
    "\n",
    "def main():\n",
    "    urls = input(\"Enter URLs (comma separated): \").split(',')\n",
    "    combined_content = ' '.join([read_web_page(url.strip()) for url in urls])\n",
    "    \n",
    "    if not combined_content.strip():\n",
    "        print(\"No valid content fetched from the provided URLs.\")\n",
    "        return\n",
    "    \n",
    "    #summary = summarize_content(combined_content)\n",
    "    #print(f\"Summary of the content: {summary}\")\n",
    "    \n",
    "    cache = {}\n",
    "    \n",
    "    with open(\"questions_and_answers.txt\", \"a\") as file:\n",
    "        while True:\n",
    "            question = input(\"Enter your question: \")\n",
    "            \n",
    "            if question in cache:\n",
    "                answer = cache[question]\n",
    "            else:\n",
    "                answer = answer_questions(combined_content, question)\n",
    "                cache[question] = answer\n",
    "            \n",
    "            print(f\"Answer: {answer}\")\n",
    "            \n",
    "            file.write(f\"Question: {question}\\nAnswer: {answer}\\n\\n\")\n",
    "            \n",
    "            continue_prompt = input(\"Do you want to ask another question? (yes/no): \").strip().lower()\n",
    "            if continue_prompt != 'yes':\n",
    "                break\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Improved caching\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from ollama import Client\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "from collections import OrderedDict\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "CACHE_FILE = \"cache.json\"\n",
    "CACHE_EXPIRY_HOURS = 24\n",
    "\n",
    "# Function to read web page content\n",
    "def read_web_page(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # Remove unnecessary elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "        return soup.get_text()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching the URL: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Function to answer questions using ollama\n",
    "def answer_questions(content, question):\n",
    "    client = Client(host='http://localhost:11434')\n",
    "    chat_prompt = [{'role': 'user', 'content': f\"Here is some content: {content}\\n\\nBased on this content, please answer the following question: {question}\\n\\nAnswer:\"}]\n",
    "    response = client.chat(model='llama3.2', messages=chat_prompt)\n",
    "    return response['message']['content']\n",
    "\n",
    "# Function to load cache from file\n",
    "def load_cache():\n",
    "    if os.path.exists(CACHE_FILE):\n",
    "        with open(CACHE_FILE, 'r') as file:\n",
    "            return json.load(file, object_pairs_hook=OrderedDict)\n",
    "    return OrderedDict()\n",
    "\n",
    "# Function to save cache to file\n",
    "def save_cache(cache):\n",
    "    with open(CACHE_FILE, 'w') as file:\n",
    "        json.dump(cache, file)\n",
    "\n",
    "# Function to clean expired cache entries\n",
    "def clean_cache(cache):\n",
    "    now = datetime.now()\n",
    "    keys_to_delete = [key for key, value in cache.items() if datetime.fromisoformat(value['timestamp']) < now - timedelta(hours=CACHE_EXPIRY_HOURS)]\n",
    "    for key in keys_to_delete:\n",
    "        del cache[key]\n",
    "\n",
    "def main():\n",
    "    urls = input(\"Enter URLs (comma separated): \").split(',')\n",
    "    combined_content = ' '.join([read_web_page(url.strip()) for url in urls])\n",
    "    \n",
    "    if not combined_content.strip():\n",
    "        print(\"No valid content fetched from the provided URLs.\")\n",
    "        return\n",
    "    \n",
    "    cache = load_cache()\n",
    "    clean_cache(cache)\n",
    "    \n",
    "    content_hash = hashlib.md5(combined_content.encode()).hexdigest()\n",
    "    \n",
    "    with open(\"questions_and_answers.txt\", \"a\") as file:\n",
    "        while True:\n",
    "            question = input(\"Enter your question: \")\n",
    "            cache_key = f\"{content_hash}_{question}\"\n",
    "            \n",
    "            if cache_key in cache:\n",
    "                answer = cache[cache_key]['answer']\n",
    "            else:\n",
    "                answer = answer_questions(combined_content, question)\n",
    "                cache[cache_key] = {'answer': answer, 'timestamp': datetime.now().isoformat()}\n",
    "                save_cache(cache)\n",
    "            \n",
    "            print(f\"Answer: {answer}\")\n",
    "            \n",
    "            file.write(f\"Question: {question}\\nAnswer: {answer}\\n\\n\")\n",
    "            \n",
    "            continue_prompt = input(\"Do you want to ask another question? (yes/no): \").strip().lower()\n",
    "            if continue_prompt != 'yes':\n",
    "                break\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vector db embedding\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from ollama import Client\n",
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Function to read web page content\n",
    "def read_web_page(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # Remove unnecessary elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "        return soup.get_text()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching the URL: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Function to answer questions using ollama\n",
    "def answer_questions(content, question):\n",
    "    client = Client(host='http://localhost:11434')\n",
    "    chat_prompt = [{'role': 'user', 'content': f\"Here is some content: {content}\\n\\nBased on this content, please answer the following question: {question}\\n\\nAnswer:\"}]\n",
    "    response = client.chat(model='llama3.2', messages=chat_prompt)\n",
    "    return response['message']['content']\n",
    "\n",
    "# Function to generate embeddings\n",
    "def generate_embeddings(texts, model):\n",
    "    return model.encode(texts)\n",
    "\n",
    "# Function to search for relevant content using FAISS\n",
    "def search_faiss(index, query_embedding, texts, k=5):\n",
    "    D, I = index.search(query_embedding, k)\n",
    "    return [texts[i] for i in I[0]]\n",
    "\n",
    "def main():\n",
    "    urls = input(\"Enter URLs (comma separated): \").split(',')\n",
    "    combined_content = ' '.join([read_web_page(url.strip()) for url in urls])\n",
    "    \n",
    "    if not combined_content.strip():\n",
    "        print(\"No valid content fetched from the provided URLs.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize the sentence transformer model\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    # Generate embeddings for the combined content\n",
    "    content_sentences = combined_content.split('. ')\n",
    "    content_embeddings = generate_embeddings(content_sentences, model)\n",
    "    \n",
    "    # Create a FAISS index and add the embeddings\n",
    "    dimension = content_embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(content_embeddings)\n",
    "    \n",
    "    with open(\"questions_and_answers.txt\", \"a\") as file:\n",
    "        while True:\n",
    "            question = input(\"Enter your question: \")\n",
    "            \n",
    "            # Generate embedding for the question\n",
    "            question_embedding = generate_embeddings([question], model)\n",
    "            \n",
    "            # Search for relevant content using FAISS\n",
    "            relevant_content = search_faiss(index, question_embedding, content_sentences)\n",
    "            combined_relevant_content = ' '.join(relevant_content)\n",
    "            \n",
    "            # Answer the question using the relevant content\n",
    "            answer = answer_questions(combined_relevant_content, question)\n",
    "            print(f\"Answer: {answer}\")\n",
    "            \n",
    "            file.write(f\"Question: {question}\\nAnswer: {answer}\\n\\n\")\n",
    "            \n",
    "            continue_prompt = input(\"Do you want to ask another question? (yes/no): \").strip().lower()\n",
    "            if continue_prompt != 'yes':\n",
    "                break\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prebuilding seperately\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "\n",
    "# Function to read web page content\n",
    "def read_web_page(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # Remove unnecessary elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "        return soup.get_text()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching the URL: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Function to generate embeddings\n",
    "def generate_embeddings(texts, model):\n",
    "    return model.encode(texts)\n",
    "\n",
    "def main():\n",
    "    urls = input(\"Enter URLs (comma separated): \").split(',')\n",
    "    combined_content = ' '.join([read_web_page(url.strip()) for url in urls])\n",
    "    \n",
    "    if not combined_content.strip():\n",
    "        print(\"No valid content fetched from the provided URLs.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize the sentence transformer model\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    # Generate embeddings for the combined content\n",
    "    content_sentences = combined_content.split('. ')\n",
    "    content_embeddings = generate_embeddings(content_sentences, model)\n",
    "    \n",
    "    # Create a FAISS index and add the embeddings\n",
    "    dimension = content_embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(content_embeddings)\n",
    "    \n",
    "    # Save the index and content sentences\n",
    "    faiss.write_index(index, \"vector_index.faiss\")\n",
    "    with open(\"content_sentences.json\", \"w\") as f:\n",
    "        json.dump(content_sentences, f)\n",
    "    \n",
    "    print(\"Vector database prebuilt and saved.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading and doing q and a\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from ollama import Client\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "\n",
    "# Function to read web page content\n",
    "def read_web_page(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # Remove unnecessary elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "        return soup.get_text()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching the URL: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Function to answer questions using ollama\n",
    "def answer_questions(content, question):\n",
    "    client = Client(host='http://localhost:11434')\n",
    "    chat_prompt = [{'role': 'user', 'content': f\"Here is some content: {content}\\n\\nBased on this content, please answer the following question: {question}\\n\\nAnswer:\"}]\n",
    "    response = client.chat(model='llama3.2', messages=chat_prompt)\n",
    "    return response['message']['content']\n",
    "\n",
    "# Function to generate embeddings\n",
    "def generate_embeddings(texts, model):\n",
    "    return model.encode(texts)\n",
    "\n",
    "# Function to search for relevant content using FAISS\n",
    "def search_faiss(index, query_embedding, texts, k=5):\n",
    "    D, I = index.search(query_embedding, k)\n",
    "    return [texts[i] for i in I[0]]\n",
    "\n",
    "def main():\n",
    "    # Load the prebuilt FAISS index and content sentences\n",
    "    index = faiss.read_index(\"vector_index.faiss\")\n",
    "    with open(\"content_sentences.json\", \"r\") as f:\n",
    "        content_sentences = json.load(f)\n",
    "    \n",
    "    # Initialize the sentence transformer model\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    with open(\"questions_and_answers.txt\", \"a\") as file:\n",
    "        while True:\n",
    "            question = input(\"Enter your question: \")\n",
    "            \n",
    "            # Generate embedding for the question\n",
    "            question_embedding = generate_embeddings([question], model)\n",
    "            \n",
    "            # Search for relevant content using FAISS\n",
    "            relevant_content = search_faiss(index, question_embedding, content_sentences)\n",
    "            combined_relevant_content = ' '.join(relevant_content)\n",
    "            \n",
    "            # Answer the question using the relevant content\n",
    "            answer = answer_questions(combined_relevant_content, question)\n",
    "            print(f\"Answer: {answer}\")\n",
    "            \n",
    "            file.write(f\"Question: {question}\\nAnswer: {answer}\\n\\n\")\n",
    "            \n",
    "            continue_prompt = input(\"Do you want to ask another question? (yes/no): \").strip().lower()\n",
    "            if continue_prompt != 'yes':\n",
    "                break\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confluence\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from ollama import Client\n",
    "\n",
    "# Function to read Confluence page content\n",
    "def read_confluence_page(url, username, api_token):\n",
    "    headers = {\n",
    "        'Authorization': f'Basic {username}:{api_token}',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    content = data['body']['storage']['value']\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    return soup.get_text()\n",
    "\n",
    "# Function to answer questions using Llama\n",
    "def answer_questions(content, question):\n",
    "    client = Client(host='http://localhost:11434')\n",
    "    chat_prompt = [{'role': 'user', 'content': f\"Here is some content: {content}\\n\\nBased on this content, please answer the following question: {question}\\n\\nAnswer:\"}]\n",
    "    response = client.chat(model='llama3', messages=chat_prompt)\n",
    "    return response['message']['content']\n",
    "\n",
    "def main():\n",
    "    urls = input(\"Enter Confluence URLs (comma separated): \").split(',')\n",
    "    username = input(\"Enter your Confluence username: \")\n",
    "    api_token = input(\"Enter your Confluence API token: \")\n",
    "    \n",
    "    combined_content = ' '.join([read_confluence_page(url.strip(), username, api_token) for url in urls])\n",
    "    \n",
    "    if not combined_content.strip():\n",
    "        print(\"No valid content fetched from the provided URLs.\")\n",
    "        return\n",
    "    \n",
    "    while True:\n",
    "        question = input(\"Enter your question: \")\n",
    "        answer = answer_questions(combined_content, question)\n",
    "        print(f\"Answer: {answer}\")\n",
    "        \n",
    "        continue_prompt = input(\"Do you want to ask another question? (yes/no): \").strip().lower()\n",
    "        if continue_prompt != 'yes':\n",
    "            break\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
