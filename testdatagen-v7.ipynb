{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-requisites folder creation\n",
    "#pip install pandas numpy glob os string tkinter faker csv json glob\n",
    "\n",
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "base_location = os.getcwd()\n",
    "\n",
    "# Directory names to be created\n",
    "directory_names = ['1.schema', '2.output', '3.master', '4.updated', '5.json', '6.sql']\n",
    "\n",
    "# Create directories using the current location as base location\n",
    "for dir_name in directory_names:\n",
    "    dir_path = os.path.join(base_location, dir_name)\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    print(f\"Directory '{dir_name}' created at '{dir_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Generate Test Data Based on Schema\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import random\n",
    "import string\n",
    "import os\n",
    "from tkinter import Tk, filedialog\n",
    "from faker import Faker\n",
    "\n",
    "def browse_directory(prompt,default_path):\n",
    "    Tk().withdraw()  # We don't want a full GUI, so keep the root window from appearing\n",
    "    directory = filedialog.askdirectory(title=prompt,initialdir=default_path)\n",
    "    return directory\n",
    "\n",
    "# Set the default path for ease of use\n",
    "default_schema_directory = os.path.join(os.getcwd(), \"1.schema\")\n",
    "default_output_directory = os.path.join(os.getcwd(), \"2.output\")\n",
    "\n",
    "# Get user input for the directory containing the schema files\n",
    "schema_directory = browse_directory(\"Select the directory containing the schema CSV files\",default_schema_directory)\n",
    "\n",
    "# Get user input for the directory containing the output files\n",
    "output_directory = browse_directory(\"Select the directory to save the test data CSV files\",default_output_directory)\n",
    "\n",
    "faker = Faker()\n",
    "\n",
    "# Function to generate random string of a given length\n",
    "def random_string(length):\n",
    "    return ''.join(random.choices(string.ascii_lowercase, k=length))\n",
    "\n",
    "# Function to generate random integer with a specific number of digits\n",
    "def random_integer(min_digits, max_digits):\n",
    "    min_value = 10**(min_digits - 1)\n",
    "    max_value = 10**max_digits - 1\n",
    "    return np.random.randint(min_value, max_value + 1, dtype=np.int64)\n",
    "\n",
    "# Function to generate unique IDs\n",
    "def generate_unique_ids(num_rows):\n",
    "    return list(range(1, num_rows + 1))\n",
    "\n",
    "# Function to generate data based on schema\n",
    "def generate_data(schema, num_rows):\n",
    "    data = {}\n",
    "    for index, row in schema.iterrows():\n",
    "        col = row[0]\n",
    "        dtype_info = row[1]\n",
    "        is_primary_key = len(row) > 2 and row[2] == 'primary_key'\n",
    "        dtype_parts = dtype_info.split(':')\n",
    "        dtype = dtype_parts[0]\n",
    "        min_length = int(dtype_parts[1]) if len(dtype_parts) > 1 else None\n",
    "        max_length = int(dtype_parts[2]) if len(dtype_parts) > 2 else None\n",
    "        permissible_values = dtype_parts[3].split('|') if len(dtype_parts) > 3 else None\n",
    "        \n",
    "        if is_primary_key:\n",
    "            data[col] = generate_unique_ids(num_rows)\n",
    "        elif permissible_values:\n",
    "            data[col] = np.random.choice(permissible_values, num_rows)\n",
    "        elif dtype == 'int':\n",
    "            if min_length and max_length:\n",
    "                data[col] = [random_integer(min_length, max_length) for _ in range(num_rows)]\n",
    "            else:\n",
    "                data[col] = np.random.randint(1, 100, num_rows)\n",
    "        elif dtype == 'float':\n",
    "            data[col] = np.random.rand(num_rows) * 100\n",
    "        elif dtype == 'name':\n",
    "            data[col] = [faker.name() for _ in range(num_rows)]\n",
    "        elif dtype == 'date':\n",
    "            data[col] = [faker.date() for _ in range(num_rows)]\n",
    "        elif dtype == 'datetime':\n",
    "            data[col] = [faker.date_time().isoformat() for _ in range(num_rows)]\n",
    "        elif dtype == 'city':\n",
    "            data[col] = [faker.city() for _ in range(num_rows)]\n",
    "        elif dtype == 'country':\n",
    "            data[col] = [faker.country() for _ in range(num_rows)]\n",
    "        elif dtype == 'state':\n",
    "            data[col] = [faker.state() for _ in range(num_rows)]\n",
    "        elif dtype == 'zip':\n",
    "            data[col] = [faker.zipcode() for _ in range(num_rows)]\n",
    "        elif dtype == 'address_line1':\n",
    "            data[col] = [faker.street_address() for _ in range(num_rows)]\n",
    "        elif dtype == 'address_line2':\n",
    "            data[col] = [faker.secondary_address() for _ in range(num_rows)]\n",
    "        elif dtype == 'bool_1_0':\n",
    "            data[col] = np.random.choice([1, 0], num_rows)\n",
    "        elif dtype == 'str':\n",
    "            if min_length and max_length:\n",
    "                lengths = np.random.randint(min_length, max_length + 1, num_rows)\n",
    "                data[col] = [random_string(length) for length in lengths]\n",
    "            elif min_length:\n",
    "                data[col] = [random_string(min_length) for _ in range(num_rows)]\n",
    "            else:\n",
    "                data[col] = np.random.choice(['A', 'B', 'C'], num_rows)\n",
    "        else:\n",
    "            data[col] = np.random.choice(['unknown'], num_rows)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Load all transposed schema files\n",
    "schema_files = glob.glob(os.path.join(schema_directory, 'schema_*.csv'))  # Adjust the pattern if needed\n",
    "# Generate and save test data for each schema file\n",
    "num_rows = 50\n",
    "for schema_file in schema_files:\n",
    "    schema = pd.read_csv(schema_file, header=None)\n",
    "    test_data = generate_data(schema, num_rows)\n",
    "    output_file = os.path.join(output_directory, os.path.basename(schema_file).replace('schema_', ''))  # Create corresponding output file name\n",
    "    test_data.to_csv(output_file, index=False)\n",
    "    print(f\"Test data generated and saved to '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Generate Master Data for Party, Account, Party Account Bridge\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tkinter import Tk, filedialog\n",
    "\n",
    "def browse_directory(prompt,default_path):\n",
    "    Tk().withdraw()  # We don't want a full GUI, so keep the root window from appearing\n",
    "    directory = filedialog.askdirectory(title=prompt,initialdir=default_path)\n",
    "    return directory\n",
    "\n",
    "# Set the default path for ease of use\n",
    "default_master_directory = os.path.join(os.getcwd(), \"3.master\")\n",
    "\n",
    "# Get user input for the directory to save the files\n",
    "save_directory = browse_directory(\"Select the directory to save the master CSV files\",default_master_directory)\n",
    "\n",
    "# Function to generate unique 18-digit party numbers\n",
    "def generate_unique_party_numbers(num_rows):\n",
    "    unique_numbers = set()\n",
    "    while len(unique_numbers) < num_rows:\n",
    "        unique_numbers.add(str(np.random.randint(10**17, 10**18, dtype=np.int64)))\n",
    "    return list(unique_numbers)\n",
    "\n",
    "# Function to generate unique 10-digit account numbers\n",
    "def generate_unique_account_numbers(num_rows):\n",
    "    unique_numbers = set()\n",
    "    while len(unique_numbers) < num_rows:\n",
    "        unique_numbers.add(str(np.random.randint(10**9, 10**10, dtype=np.int64)))\n",
    "    return list(unique_numbers)\n",
    "\n",
    "# Function to define roles\n",
    "def define_roles(num_rows):\n",
    "    roles = ['Primary', 'Secondary']\n",
    "    return np.random.choice(roles, num_rows)\n",
    "\n",
    "\n",
    "# Number of rows for each file\n",
    "num_rows_account = 50\n",
    "num_rows_party = num_rows_account * 2\n",
    "\n",
    "# Generate data\n",
    "party_numbers = generate_unique_party_numbers(num_rows_party)\n",
    "account_numbers = generate_unique_account_numbers(num_rows_account)\n",
    "roles = define_roles(num_rows_account)\n",
    "\n",
    "# Create DataFrames\n",
    "df_party = pd.DataFrame({'party_num': party_numbers})\n",
    "df_account = pd.DataFrame({'acct_num': account_numbers})\n",
    "df_role = pd.DataFrame({'role': roles})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Save DataFrames to CSV files\n",
    "df_party.to_csv(os.path.join(save_directory,'master_party_numbers.csv'), index=False)\n",
    "df_account.to_csv(os.path.join(save_directory,'master_account_numbers.csv'), index=False)\n",
    "#df_role.to_csv(os.path.join(save_directory,'master_roles.csv'), index=False)\n",
    "\n",
    "# Combine data into a single DataFrame ensuring no account, role combination have the same customer\n",
    "combined_data = []\n",
    "used_combinations = set()\n",
    "\n",
    "for i in range(num_rows_account):\n",
    "    account = account_numbers[i]\n",
    "    role = roles[i]\n",
    "    customer = party_numbers[i]\n",
    "    if (account, role) not in used_combinations:\n",
    "        combined_data.append({'Customer': customer, 'Account': account, 'Role': role})\n",
    "        used_combinations.add((account, role))\n",
    "\n",
    "# Add remaining party numbers to the combined data\n",
    "remaining_customers = set(party_numbers) - set([entry['Customer'] for entry in combined_data])\n",
    "for customer in remaining_customers:\n",
    "    account = np.random.choice(account_numbers)\n",
    "    role = np.random.choice(roles)\n",
    "    combined_data.append({'Customer': customer, 'Account': account, 'Role': role})\n",
    "\n",
    "# Ensure all account numbers are used at least once\n",
    "used_accounts = set([entry['Account'] for entry in combined_data])\n",
    "remaining_accounts = set(account_numbers) - used_accounts\n",
    "for account in remaining_accounts:\n",
    "    customer = np.random.choice(party_numbers)\n",
    "    role = np.random.choice(roles)\n",
    "    combined_data.append({'Customer': customer, 'Account': account, 'Role': role})\n",
    "\n",
    "# Ensure all party numbers are used at least once\n",
    "used_customers = set([entry['Customer'] for entry in combined_data])\n",
    "remaining_customers = set(party_numbers) - used_customers\n",
    "for customer in remaining_customers:\n",
    "    account = np.random.choice(account_numbers)\n",
    "    role = np.random.choice(roles)\n",
    "    combined_data.append({'Customer': customer, 'Account': account, 'Role': role})\n",
    "\n",
    "df_combined = pd.DataFrame(combined_data)\n",
    "\n",
    "# Save the combined DataFrame to a CSV file\n",
    "#df_combined.to_csv('master_party_account_role.csv', index=False)\n",
    "\n",
    "print(f\"Master data generated and saved to '{save_directory}'\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Replace Party, Account, Party Account with Master Data for joining on key columns\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import shutil\n",
    "from tkinter import Tk, filedialog\n",
    "\n",
    "\n",
    "# Function to browse and select multiple files\n",
    "def browse_files(prompt,default_path):\n",
    "    Tk().withdraw()  # We don't want a full GUI, so keep the root window from appearing\n",
    "    file_paths = filedialog.askopenfilenames(title=prompt, initialdir=default_path, filetypes=[(\"CSV files\", \"*.csv\")])\n",
    "    return file_paths\n",
    "\n",
    "# Set the default path for ease of use\n",
    "default_output_directory = os.path.join(os.getcwd(), \"2.output\")\n",
    "default_master_directory = os.path.join(os.getcwd(), \"3.master\")\n",
    "default_updated_directory = os.path.join(os.getcwd(), \"4.updated\")\n",
    "\n",
    "\n",
    "# Get user input for the master and target files\n",
    "master_party_file = filedialog.askopenfilename(title=\"Select the master party CSV file\",  filetypes=[(\"CSV files\", \"*.csv\")], initialdir=default_master_directory)\n",
    "master_account_file = filedialog.askopenfilename(title=\"Select the master account CSV file\", filetypes=[(\"CSV files\", \"*.csv\")],initialdir=default_master_directory)\n",
    "target_files = browse_files(\"Select the target CSV files\",default_output_directory)\n",
    "\n",
    "# Load the master files\n",
    "df_master_party = pd.read_csv(master_party_file, dtype={'party_num': str})\n",
    "df_master_account = pd.read_csv(master_account_file, dtype={'acct_num': str})\n",
    "\n",
    "# Convert column names to lowercase\n",
    "df_master_party.columns = df_master_party.columns.str.lower()\n",
    "df_master_account.columns = df_master_account.columns.str.lower()\n",
    "\n",
    "# Function to update target files with referential integrity\n",
    "def update_target_file(target_file, key_column, master_df):\n",
    "    df_target = pd.read_csv(target_file, dtype={key_column: str})\n",
    "    df_target.columns = df_target.columns.str.lower()\n",
    "    df_unique_target = df_target.drop_duplicates(subset=[key_column])\n",
    "    df_unique_master = master_df.drop_duplicates(subset=[key_column])\n",
    "\n",
    "    if len(df_unique_target) <= len(df_unique_master):\n",
    "        df_unique_target[key_column] = df_unique_master[key_column].values[:len(df_unique_target)]\n",
    "    else:\n",
    "        raise ValueError(f\"Target count is greater than master count for {target_file}. Replacement not possible.\")\n",
    "\n",
    "    output_file = os.path.join(os.path.dirname(target_file), 'updated_' + os.path.basename(target_file))\n",
    "    df_unique_target.to_csv(output_file, index=False)\n",
    "    print(f\"Column values replaced successfully! The updated file is saved as '{output_file}'\")\n",
    "    return df_unique_target\n",
    "# Update each target file\n",
    "for target_file in target_files:\n",
    "    if 'party' in os.path.basename(target_file).lower() and 'acct' in os.path.basename(target_file).lower():\n",
    "        # doing for party\n",
    "        df_updated = update_target_file(target_file, 'party_num', df_master_party)\n",
    "        output_file = os.path.join(os.path.dirname(target_file), 'updated_' + os.path.basename(target_file))\n",
    "        df_updated.to_csv(output_file, index=False)\n",
    "        print(f\"Party column values replaced successfully! The intermediate file is saved as '{output_file}'\")\n",
    "        # doing for acct\n",
    "        df_updated = update_target_file(output_file, 'acct_num', df_master_account)\n",
    "        df_updated.to_csv(output_file, index=False)\n",
    "        print(f\"Account column values replaced successfully! The final updated file is saved as '{output_file}'\")\n",
    "    if 'party' in os.path.basename(target_file).lower():\n",
    "        update_target_file(target_file, 'party_num', df_master_party)\n",
    "    elif 'acct' in os.path.basename(target_file).lower():\n",
    "        update_target_file(target_file, 'acct_num', df_master_account)\n",
    "\n",
    "\n",
    "# Get user input for the directory containing the updated files\n",
    "source_directory = browse_directory(\"Select the directory containing the updated files\",default_output_directory)\n",
    "# Get user input for the target directory to move the updated files\n",
    "target_directory = browse_directory(\"Select the directory to move the updated files\",default_updated_directory)\n",
    "# Find all files with the prefix 'updated_' in the source directory\n",
    "updated_files = [f for f in os.listdir(source_directory) if f.startswith('updated_')]\n",
    "\n",
    "# Move each updated file to the target directory\n",
    "for file_name in updated_files:\n",
    "    source_path = os.path.join(source_directory, file_name)\n",
    "    target_path = os.path.join(target_directory, file_name)\n",
    "    shutil.move(source_path, target_path)\n",
    "    print(f\"Moved '{file_name}' to '{target_directory}'\")\n",
    "\n",
    "print(\"All updated files have been successfully moved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv to json\n",
    "import csv\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "from tkinter import Tk, filedialog\n",
    "\n",
    "def browse_directory(prompt,default_path):\n",
    "    Tk().withdraw()  # We don't want a full GUI, so keep the root window from appearing\n",
    "    directory = filedialog.askdirectory(title=prompt,initialdir=default_path)\n",
    "    return directory\n",
    "\n",
    "# Set the default path for ease of use\n",
    "default_csv_directory = os.path.join(os.getcwd(), \"4.updated\")\n",
    "default_json_directory = os.path.join(os.getcwd(), \"5.json\")\n",
    "\n",
    "# Get user input for the directory containing the CSV files\n",
    "csv_directory = browse_directory(\"Select the directory containing the CSV files\",default_csv_directory)\n",
    "\n",
    "# Path to your CSV files\n",
    "csv_files = glob.glob(os.path.join(csv_directory, '*.csv'))\n",
    "\n",
    "# Get user input for the target directory to save JSON files\n",
    "json_directory = browse_directory(\"Select the directory to save the JSON files\",default_json_directory)\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    # Read the CSV file\n",
    "    with open(csv_file, mode='r', newline='') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        rows = list(reader)\n",
    "\n",
    "    # Convert to JSON\n",
    "    json_data = json.dumps(rows, indent=4)\n",
    "\n",
    "    # Write to a JSON file in the selected directory\n",
    "    json_file_name = os.path.splitext(os.path.basename(csv_file))[0].replace('updated_', '') + '.json'\n",
    "    json_file_path = os.path.join(json_directory, json_file_name)\n",
    "    with open(json_file_path, mode='w') as json_file:\n",
    "        json_file.write(json_data)\n",
    "\n",
    "    print(f\"Converted {csv_file} to {json_file_path}\")\n",
    "\n",
    "print(\"All CSV files have been successfully converted to JSON!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Insert Sql with json\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "from tkinter import Tk, filedialog\n",
    "\n",
    "# Function to browse and select a directory\n",
    "def browse_directory(prompt,default_path):\n",
    "    Tk().withdraw()  # We don't want a full GUI, so keep the root window from appearing\n",
    "    directory = filedialog.askdirectory(title=prompt,initialdir=default_path)\n",
    "    return directory\n",
    "\n",
    "# Set the default path for ease of use\n",
    "default_json_directory = os.path.join(os.getcwd(), \"5.json\")\n",
    "default_sql_directory = os.path.join(os.getcwd(), \"6.sql\")\n",
    "\n",
    "# Get user input for the directory containing the JSON files\n",
    "json_directory = browse_directory(\"Select the directory containing the JSON files\",default_json_directory)\n",
    "\n",
    "# Get user input for the target directory to save SQL files\n",
    "output_directory = browse_directory(\"Select the directory to save the SQL files\",default_sql_directory)\n",
    "\n",
    "# Directory containing JSON files and the wildcard pattern\n",
    "json_files_pattern = os.path.join(json_directory, '*.json')\n",
    "\n",
    "# Find all JSON files matching the pattern\n",
    "input_json_files = glob.glob(json_files_pattern)\n",
    "\n",
    "# Data for non-JSON columns\n",
    "BATCH_ID = '123456789'\n",
    "LINE_OF_BUSINESS = 'MORTGAGE'\n",
    "SNAPSHOT_DT = '12/31/2024'\n",
    "\n",
    "# Iterate over each JSON file\n",
    "for input_json_file in input_json_files:\n",
    "    # Read JSON data from the input file\n",
    "    with open(input_json_file, 'r') as file:\n",
    "        json_data = json.load(file)\n",
    "\n",
    "    table_name = os.path.splitext(os.path.basename(input_json_file))[0]\n",
    "    # Output file path\n",
    "    output_file_path = os.path.join(output_directory, f\"{table_name}.sql\")\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(os.path.dirname(output_file_path), exist_ok=True)\n",
    "\n",
    "    # Open the output file for writing\n",
    "    with open(output_file_path, 'w') as output_file:\n",
    "        # Iterate over each record in the JSON data\n",
    "        for record in json_data:\n",
    "            # Convert the current record to a JSON string\n",
    "            json_str = json.dumps(record)\n",
    "\n",
    "            # Generate the insert statement for the current record\n",
    "            insert_sql = f\"\"\"\n",
    "            INSERT INTO {table_name} (BATCH_ID, LINE_OF_BUSINESS, SNAPSHOT_DT, OBJECT_NAME, JSON_MESSAGE)\n",
    "            VALUES ('{BATCH_ID}', '{LINE_OF_BUSINESS}', '{SNAPSHOT_DT}', '{json_str}');\n",
    "            \"\"\"\n",
    "\n",
    "            # Write the insert statement to the output file\n",
    "            output_file.write(insert_sql + \"\\n\")\n",
    "\n",
    "    print(f\"Insert statements have been written to {output_file_path}\")\n",
    "\n",
    "print(\"All JSON files have been successfully converted to SQL insert statements!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OLD WAY CONVERT TO JSON\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "\n",
    "def csv_to_json(csv_file_path, json_file_path):\n",
    "    with open(csv_file_path, mode='r', encoding='utf-8') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        \n",
    "        with open(json_file_path, mode='w', encoding='utf-8') as json_file:\n",
    "            for row in csv_reader:\n",
    "                json.dump(row, json_file)\n",
    "                json_file.write('\\n')\n",
    "\n",
    "def convert_all_csv_to_json(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.csv'):\n",
    "            csv_file_path = os.path.join(directory, filename)\n",
    "            json_file_path = os.path.join(directory, filename.replace('.csv', '.json'))\n",
    "            csv_to_json(csv_file_path, json_file_path)\n",
    "            print(f'Converted {csv_file_path} to {json_file_path}')\n",
    "\n",
    "# Replace 'your_directory_path' with the path to your directory containing CSV files\n",
    "convert_all_csv_to_json('output')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "README\n",
    ">>1.Generate Test Data Files based on schema\n",
    ">>2.Generate Master Files Party, Account with record count greater than the mock data files\n",
    ">>3.Replace Party , Account Number Details with Master Files to enforce Party, Account, Party Account are joined\n",
    ">>4.Convert csv to json\n",
    ">>5.Add non json fields + json field in a sql statement\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
